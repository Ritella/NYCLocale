{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze subway turnstile data for the last 6 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime, timedelta\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import geocoder\n",
    "import geoplot\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "main_path = '/Users/rita/Dropbox/InsightDataScience/Project'\n",
    "api_key = open(main_path + '/config.py', 'r')\n",
    "api_key = api_key.read().replace('api_key = ', '').replace('“','').replace('”','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C/A</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>SCP</th>\n",
       "      <th>STATION</th>\n",
       "      <th>LINENAME</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>DESC</th>\n",
       "      <th>ENTRIES_CUM</th>\n",
       "      <th>EXITS_CUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>01/26/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6922652</td>\n",
       "      <td>2347673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>01/26/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6922669</td>\n",
       "      <td>2347688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>01/26/2019</td>\n",
       "      <td>11:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6922747</td>\n",
       "      <td>2347773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>01/26/2019</td>\n",
       "      <td>15:00:00</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>6922932</td>\n",
       "      <td>2347849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>01/26/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6923237</td>\n",
       "      <td>2347911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C/A  UNIT       SCP STATION LINENAME DIVISION        DATE      TIME  \\\n",
       "0  A002  R051  02-00-00   59 ST  NQR456W      BMT  01/26/2019  03:00:00   \n",
       "1  A002  R051  02-00-00   59 ST  NQR456W      BMT  01/26/2019  07:00:00   \n",
       "2  A002  R051  02-00-00   59 ST  NQR456W      BMT  01/26/2019  11:00:00   \n",
       "3  A002  R051  02-00-00   59 ST  NQR456W      BMT  01/26/2019  15:00:00   \n",
       "4  A002  R051  02-00-00   59 ST  NQR456W      BMT  01/26/2019  19:00:00   \n",
       "\n",
       "         DESC  ENTRIES_CUM  EXITS_CUM  \n",
       "0     REGULAR      6922652    2347673  \n",
       "1     REGULAR      6922669    2347688  \n",
       "2     REGULAR      6922747    2347773  \n",
       "3  RECOVR AUD      6922932    2347849  \n",
       "4     REGULAR      6923237    2347911  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets and append\n",
    "\n",
    "def load_and_append_txts(dates):\n",
    "    path = '/Users/rita/Dropbox/InsightDataScience/Project/Data/Turnstile'\n",
    "    df = pd.DataFrame()\n",
    "    for date in dates:\n",
    "        fn = path + '/turnstile_' + str(date) + '.txt'\n",
    "        df_new = pd.read_csv(fn)\n",
    "        df = df.append(df_new)\n",
    "    # preliminary cleaning\n",
    "    df = df.rename(columns={'ENTRIES' : 'ENTRIES_CUM',\n",
    "                            'EXITS                                                               ' \n",
    "                            : 'EXITS_CUM'})\n",
    "    return df\n",
    "\n",
    "df_1 = load_and_append_txts([190202, 190126, 190119, 190112, 190105, 181229])\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to outer borough stations\n",
    "nyc_subway_map = pd.read_csv(main_path + '/Data/Turnstile/Stations.csv')\n",
    "nyc_borough_map = gpd.read_file(main_path + '/Data/Locations/Boroughs/geo_export_042a7af9-2558-430b-b512-5799557e5b47.shp')\n",
    "nyc_subway_trnstl_template =  pd.read_csv(main_path + '/Data/Turnstile/template.txt').loc[:,['STATION', \n",
    "                                            'LINENAME']].drop_duplicates().reset_index()\n",
    "\n",
    "def strip_station_names(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace(' ', '')  \n",
    "    return s\n",
    "    \n",
    "nyc_subway_trnstl_template['StationStrip'] = nyc_subway_trnstl_template.STATION.map(strip_station_names)\n",
    "\n",
    "nyc_subway_map['StationStrip'] = nyc_subway_map['Stop Name'].map(strip_station_names)\n",
    "nyc_subway_map['LINENAME'] = nyc_subway_map['Daytime Routes'].map(lambda s: s.replace(' ', ''))\n",
    "\n",
    "# now match stations to get best estimate of lat/long\n",
    "def match_station_names(s):\n",
    "    station_name_list = nyc_subway_map.STATION_LINE\n",
    "    new_name = process.extractOne(s, station_name_list) # scorer=fuzz.partial_token_set_ratio you could change\n",
    "    return new_name[0]\n",
    "\n",
    "nyc_subway_trnstl_template['STATION_LINE'] = nyc_subway_trnstl_template.StationStrip + \\\n",
    "                                                nyc_subway_trnstl_template.LINENAME\n",
    "nyc_subway_map['STATION_LINE'] = nyc_subway_map.StationStrip + nyc_subway_map.LINENAME\n",
    "\n",
    "nyc_subway_trnstl_template['StationNameMatch'] = nyc_subway_trnstl_template.STATION_LINE.map(match_station_names) # not sure the best way to do this\n",
    "\n",
    "# get lat/long for each turnstile data subway station.\n",
    "nyc_subway_trnstl_template = pd.merge(nyc_subway_trnstl_template, nyc_subway_map[['Borough', 'GTFS Latitude', \n",
    "                                        'GTFS Longitude', 'STATION_LINE']], \n",
    "                                        left_on='StationNameMatch', \n",
    "                                        right_on='STATION_LINE').drop(['index', 'STATION_LINE_y', \n",
    "                                        'StationStrip'], axis=1).rename(columns={'STATION_LINE_x':\n",
    "                                        'STATION_LINE_A', 'StationNameMatch': 'STATION_LINE_B', \n",
    "                                        'STATION': 'STATION_NAME'})\n",
    "# Save Matchup to visually inspect\n",
    "nyc_subway_trnstl_template.to_csv(main_path + '/Results/subway_name_match_check.csv')\n",
    "\n",
    "# Perform Matchup corrections\n",
    "matchups_to_correct = pd.read_csv(main_path + '/Results/Matchup_Corrected.csv')\n",
    "for i in range(0, matchups_to_correct.shape[0]):\n",
    "    matched_val = nyc_subway_trnstl_template.STATION_LINE_A == matchups_to_correct.loc[i, 'STATION_LINE_A']\n",
    "    nyc_subway_trnstl_template.loc[matched_val, 'STATION_LINE_B'] = matchups_to_correct.loc[i, 'STATION_LINE_B']\n",
    "    nyc_subway_trnstl_template.loc[matched_val, 'Borough'] = matchups_to_correct.loc[i, 'Borough']\n",
    "    nyc_subway_trnstl_template.loc[matched_val, 'GTFS Latitude'] = np.float(matchups_to_correct.loc[i, 'GTFS Latitude'])\n",
    "    nyc_subway_trnstl_template.loc[matched_val, 'GTFS Longitude'] = np.float(matchups_to_correct.loc[i, 'GTFS Longitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to outer boroughs\n",
    "nyc_subway_trnstl_template = nyc_subway_trnstl_template[nyc_subway_trnstl_template.Borough.isin(['Bx', 'Bk',\n",
    "                                                        'Q', 'SI'])].reset_index(drop=True)\n",
    "nyc_subway_map = nyc_subway_map[nyc_subway_map.Borough.isin(['Bx', 'Bk', 'Q', \n",
    "                                            'SI'])].reset_index(drop=True)\n",
    "\n",
    "# changes station name to account for stations that are named the same but\n",
    "# have different lines\n",
    "nyc_subway_trnstl_template['STATION'] = nyc_subway_trnstl_template['STATION_NAME'] + ' (' \\\n",
    "                                            + nyc_subway_trnstl_template['LINENAME'] + ')'\n",
    "nyc_subway_trnstl_template = nyc_subway_trnstl_template.drop_duplicates()\n",
    "\n",
    "df_1 = df_1.rename(columns={'STATION': 'STATION_NAME'})\n",
    "df_1['STATION'] = df_1['STATION_NAME'] + ' (' + df_1['LINENAME'] + ')'\n",
    "\n",
    "nyc_subway_map = nyc_subway_map.drop(['Station ID', 'Complex ID', \n",
    "                                      'GTFS Stop ID'], axis=1).drop_duplicates(subset=['STATION_LINE'], \n",
    "                                      keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Stations of Interest (remove manhattan)\n",
    "df_1 = df_1[df_1['STATION'].isin(nyc_subway_trnstl_template.STATION)]\n",
    "\n",
    "# Filter to STD Times & convert to usable format\n",
    "std_times = [str(t).zfill(2) + ':00:00' for t in (range(0, 24, 1))] # zfill zero-pads\n",
    "df_1 = df_1[df_1['TIME'].isin(std_times)]\n",
    "df_1['TIME'] = df_1['TIME'].map(lambda x: \n",
    "                                  int(x.replace(':00', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C/A</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>SCP</th>\n",
       "      <th>STATION_NAME</th>\n",
       "      <th>LINENAME</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DESC</th>\n",
       "      <th>ENTRIES_CUM</th>\n",
       "      <th>EXITS_CUM</th>\n",
       "      <th>STATION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "      <td>40021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "      <td>55051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "      <td>40014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C/A   UNIT    SCP  STATION_NAME  LINENAME  DIVISION   DATE   DESC  \\\n",
       "TIME                                                                        \n",
       "0     40021  40021  40021         40021     40021     40021  40021  40021   \n",
       "1       294    294    294           294       294       294    294    294   \n",
       "3     55051  55051  55051         55051     55051     55051  55051  55051   \n",
       "4     40014  40014  40014         40014     40014     40014  40014  40014   \n",
       "5       287    287    287           287       287       287    287    287   \n",
       "\n",
       "      ENTRIES_CUM  EXITS_CUM  STATION  \n",
       "TIME                                   \n",
       "0           40021      40021    40021  \n",
       "1             294        294      294  \n",
       "3           55051      55051    55051  \n",
       "4           40014      40014    40014  \n",
       "5             287        287      287  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check timing counts\n",
    "df_1.groupby('TIME').count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rita/anaconda3/envs/insight/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Unroll cumulative entries and exits\n",
    "\n",
    "# Very few entries indicates a station closing\n",
    "# Negatives represent counter resets\n",
    "# There are a few unrealistically large values\n",
    "# REL: Rather than hard code as > 1500, remove outliers\n",
    "# relative to the station\n",
    "# Replace with NaN\n",
    "# REL: Reconsider this approach\n",
    "\n",
    "def outlier_removal(df, column_name, group_by_column):\n",
    "    df = df.set_value((df[column_name] <= 0) | (df[column_name] >= 2000), column_name, np.nan)\n",
    "    quartiles = df.groupby(group_by_column).quantile([0.25, 0.5, 0.75]).reset_index()\n",
    "    all_metrics = pd.DataFrame({group_by_column: quartiles[group_by_column].unique()})\n",
    "    all_metrics['MEAN'] = quartiles[quartiles.level_1 == 0.5].reset_index()[column_name]\n",
    "    all_metrics['SIG'] = 0.74 * (quartiles[quartiles.level_1 == 0.75].reset_index()[column_name] -\n",
    "                                quartiles[quartiles.level_1 == 0.25].reset_index()[column_name] )\n",
    "    all_metrics['MIN']= all_metrics['MEAN'] - 5 * all_metrics['SIG']\n",
    "    all_metrics['MAX']= all_metrics['MEAN'] + 5 * all_metrics['SIG']\n",
    "    df = pd.merge(df, all_metrics, on=group_by_column, how='inner')\n",
    "    df.loc[(df[column_name] < df['MIN']) | (df[column_name] > df['MAX']), column_name] = np.nan # sigma clipping\n",
    "    return df.drop(['MIN', 'MAX', 'MEAN', 'SIG'], axis = 1)\n",
    "\n",
    "# sort first\n",
    "df_1 = df_1.sort_values(by=['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME'])\n",
    "\n",
    "# get non-cummulative\n",
    "df_1['ENTRIES'] = df_1.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['ENTRIES_CUM'].diff()\n",
    "df_1['EXITS'] = df_1.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['EXITS_CUM'].diff()\n",
    "\n",
    "# remove outliers\n",
    "df_1 = outlier_removal(df_1, 'ENTRIES', 'STATION')\n",
    "df_1 = outlier_removal(df_1, 'EXITS', 'STATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice time correct\n",
    "\n",
    "temp = pd.DataFrame({'placeholder': [-3, -2, -1, 0]}) # setup for making missing time points\n",
    "df_1['TIME_INT'] = df_1.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['TIME'].diff()\n",
    "df_1 = (df_1.assign(key=1).merge(temp.assign(key=1), on=\"key\").drop(\"key\", axis=1))\n",
    "\n",
    "# first measure the next day appears negative\n",
    "df_1.loc[df_1['TIME_INT'] == -20, 'TIME_INT'] = 4 \n",
    "df_1.loc[df_1['TIME_INT'] == -21, 'TIME_INT'] = 3 \n",
    "df_1.loc[df_1['TIME_INT'] == -22, 'TIME_INT'] = 2 \n",
    "df_1.loc[df_1['TIME_INT'] == -23, 'TIME_INT'] = 1 \n",
    "\n",
    "# drop intervals that are too long\n",
    "df_1.loc[(df_1['TIME_INT'] < 0), 'TIME_INT'] = np.nan \n",
    "df_1.loc[(df_1['TIME_INT'] > 4), 'TIME_INT'] = np.nan\n",
    "\n",
    "# get values for each hour\n",
    "df_1['ENTRIES_ST'] = df_1['ENTRIES'] / df_1['TIME_INT'] \n",
    "df_1['EXITS_ST'] = df_1['EXITS'] / df_1['TIME_INT'] \n",
    "\n",
    "# add them in\n",
    "df_1['TIME_ST'] = df_1['TIME'] + df_1['placeholder']\n",
    "\n",
    "# remove duplicate columns\n",
    "to_remove = ((df_1['placeholder'] == -3) & (df_1['TIME_INT'] == 3)) | \\\n",
    "            ((df_1['placeholder'] <= -2) & (df_1['TIME_INT'] == 2)) | \\\n",
    "            ((df_1['placeholder'] <= -1) & (df_1['TIME_INT'] == 1))  \n",
    "df_1 = df_1[~to_remove]\n",
    "\n",
    "# change first entry of the day to last entry of previous day\n",
    "to_change = df_1['TIME_ST'] <= 0\n",
    "df_1.loc[to_change, 'DATE'] = df_1.loc[to_change, 'DATE'].map(lambda x: datetime.strftime(datetime.strptime(x, \n",
    "                                                                 '%m/%d/%Y') -  timedelta(days=1),'%m/%d/%Y'))\n",
    "df_1.loc[df_1['TIME_ST'] == 0, 'TIME_ST'] = 24\n",
    "df_1.loc[df_1['TIME_ST'] == -1, 'TIME_ST'] = 23\n",
    "df_1.loc[df_1['TIME_ST'] == -2, 'TIME_ST'] = 22\n",
    "df_1.loc[df_1['TIME_ST'] == -3, 'TIME_ST'] = 21\n",
    "df_1 = df_1.sort_values(by=['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME_ST'])\n",
    "\n",
    "#.strftime('%m/%d/%Y') to reformat datetime\n",
    "df_1['WK_DAY'] = df_1['DATE'].map(lambda x: \n",
    "                                  datetime.weekday(datetime.strptime(x, '%m/%d/%Y')))\n",
    "\n",
    "df_1['WK_DAY_STR'] = df_1['DATE'].map(lambda x: \n",
    "                                   datetime.strptime(x, '%m/%d/%Y').strftime(\"%A\"))\n",
    "\n",
    "# Add an entry/exit count column \n",
    "df_1['ENTRIES_CNT'] = df_1['ENTRIES'] / df_1['ENTRIES']\n",
    "df_1['EXITS_CNT'] = df_1['EXITS'] / df_1['EXITS']\n",
    "\n",
    "df_1 = df_1.drop(['DESC', 'ENTRIES_CUM', 'EXITS_CUM', 'TIME_INT', 'TIME', 'ENTRIES',\n",
    "                'EXITS', 'placeholder'], axis=1).rename(columns={'ENTRIES_ST': 'ENTRIES',\n",
    "                'EXITS_ST': 'EXITS', 'TIME_ST': 'TIME'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by weekday\n",
    "df_1_by_wkday = df_1.groupby(['STATION', 'WK_DAY', 'WK_DAY_STR', 'TIME']).agg({'ENTRIES':'mean', \n",
    "                             'EXITS':'mean', 'ENTRIES_CNT':'sum', 'EXITS_CNT':'sum'}).reset_index()\n",
    "\n",
    "# group wkdays by type\n",
    "df_1.loc[df_1['WK_DAY'] < 4, 'WK_DAY_TYPE'] = 1\n",
    "df_1.loc[df_1['WK_DAY'] == 4, 'WK_DAY_TYPE'] = 2\n",
    "df_1.loc[df_1['WK_DAY'] == 5, 'WK_DAY_TYPE'] = 3\n",
    "df_1.loc[df_1['WK_DAY'] == 6, 'WK_DAY_TYPE'] = 4\n",
    "df_1_by_wkday_type = df_1.groupby(['STATION', 'WK_DAY_TYPE', 'TIME']).agg({'ENTRIES':'mean', \n",
    "                                'EXITS':'mean', 'ENTRIES_CNT':'sum', 'EXITS_CNT':'sum'}).reset_index()\n",
    "\n",
    "# re-collapse so data doesn't look funny\n",
    "def agg_time_slots(df):\n",
    "    df.loc[df['TIME'] <= 4, 'TIME_AGG'] = 1\n",
    "    df.loc[(df['TIME'] > 4) & (df['TIME'] <= 8), 'TIME_AGG'] = 2\n",
    "    df.loc[(df['TIME'] > 8) & (df['TIME'] <= 12), 'TIME_AGG'] = 3\n",
    "    df.loc[(df['TIME'] > 12) & (df['TIME'] <= 16), 'TIME_AGG'] = 4\n",
    "    df.loc[(df['TIME'] > 16) & (df['TIME'] <= 20), 'TIME_AGG'] = 5\n",
    "    df.loc[(df['TIME'] > 20) & (df['TIME'] <= 24), 'TIME_AGG'] = 6\n",
    "    return df\n",
    "\n",
    "df_1_by_wkday = agg_time_slots(df_1_by_wkday)\n",
    "df_1_by_wkday = df_1_by_wkday.groupby(['STATION', 'WK_DAY', 'WK_DAY_STR', 'TIME_AGG']).sum().loc[:,['ENTRIES',\n",
    "                    'EXITS']].sort_values(by=['STATION',\n",
    "                    'WK_DAY']).reset_index().rename(columns={'TIME_AGG':'TIME'})\n",
    "\n",
    "df_1_by_wkday_type = agg_time_slots(df_1_by_wkday_type)\n",
    "df_1_by_wkday_type = df_1_by_wkday_type.groupby(['STATION', 'WK_DAY_TYPE', 'TIME_AGG']).sum().loc[:,['ENTRIES',\n",
    "                    'EXITS']].sort_values(by=['STATION',\n",
    "                    'WK_DAY_TYPE']).reset_index().rename(columns={'TIME_AGG':'TIME'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stations with NaNs\n",
    "to_keep = df_1_by_wkday_type.groupby(['STATION']).count().reset_index().loc[:,['STATION', 'ENTRIES']]\n",
    "to_keep = to_keep.rename(columns={'ENTRIES': 'COUNT'})\n",
    "df_1_by_wkday_type = df_1_by_wkday_type.merge(to_keep, on='STATION')\n",
    "df_1_by_wkday_type = df_1_by_wkday_type[df_1_by_wkday_type['COUNT'] == 24]\n",
    "\n",
    "to_keep = df_1_by_wkday.groupby(['STATION']).count().reset_index().loc[:,['STATION', 'ENTRIES']]\n",
    "to_keep = to_keep.rename(columns={'ENTRIES': 'COUNT'})\n",
    "df_1_by_wkday = df_1_by_wkday.merge(to_keep, on='STATION')\n",
    "df_1_by_wkday = df_1_by_wkday[df_1_by_wkday['COUNT'] == 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "def standardize_vals(df):\n",
    "    mean_df = df.groupby(['STATION']).mean().rename(columns={\"ENTRIES\": \"ENTRIES_MEAN\", \n",
    "                                                                    \"EXITS\": \"EXITS_MEAN\"}).reset_index().loc[:,\n",
    "                                                                    ['STATION', 'ENTRIES_MEAN', 'EXITS_MEAN']]\n",
    "    sdev_df = df.groupby(['STATION']).std().rename(columns={\"ENTRIES\": \"ENTRIES_SDEV\", \n",
    "                                                                    \"EXITS\": \"EXITS_SDEV\"}).reset_index().loc[:,\n",
    "                                                                    ['STATION', 'ENTRIES_SDEV', 'EXITS_SDEV']]\n",
    "    metrics_df = pd.concat([mean_df, sdev_df], axis=1)\n",
    "    metrics_df = metrics_df.loc[:,~metrics_df.columns.duplicated()]\n",
    "    df = pd.merge(df, metrics_df, on='STATION', how='inner')\n",
    "    df['ENTRIES'] = (df['ENTRIES'] - df['ENTRIES_MEAN']) / df['ENTRIES_SDEV']\n",
    "    df['EXITS'] = (df['EXITS'] - df['EXITS_MEAN']) / df['EXITS_SDEV']\n",
    "    df = df.drop(['ENTRIES_MEAN', 'ENTRIES_SDEV', 'EXITS_MEAN', 'EXITS_SDEV'], axis=1)\n",
    "    return df\n",
    "\n",
    "df_1_by_wkday = standardize_vals(df_1_by_wkday)\n",
    "df_1_by_wkday_type = standardize_vals(df_1_by_wkday_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of any stations with nulls\n",
    "df_1_by_wkday['NA_COUNT'] = 0\n",
    "df_1_by_wkday.loc[df_1_by_wkday.isna()['ENTRIES'], 'NA_COUNT'] += 1\n",
    "df_1_by_wkday.loc[df_1_by_wkday.isna()['ENTRIES'], 'NA_COUNT'] += 1\n",
    "to_drop = df_1_by_wkday[df_1_by_wkday.NA_COUNT > 0].STATION.unique()\n",
    "df_1_by_wkday = df_1_by_wkday[~df_1_by_wkday.STATION.isin(to_drop)]\n",
    "\n",
    "df_1_by_wkday_type['NA_COUNT'] = 0\n",
    "df_1_by_wkday_type.loc[df_1_by_wkday_type.isna()['ENTRIES'], 'NA_COUNT'] += 1\n",
    "df_1_by_wkday_type.loc[df_1_by_wkday_type.isna()['ENTRIES'], 'NA_COUNT'] += 1\n",
    "to_drop = df_1_by_wkday_type[df_1_by_wkday_type.NA_COUNT > 0].STATION.unique()\n",
    "df_1_by_wkday_type = df_1_by_wkday_type[~df_1_by_wkday_type.STATION.isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get neighborhood info\n",
    "\n",
    "# get map of Brooklyn neighborhoods\n",
    "nyc_nbhd_map = gpd.read_file(main_path + '/Data/Locations/ZillowNeighborhoods-NY/ZillowNeighborhoods-NY.shp')\n",
    "nyc_subway_trnstl_template['geometry'] = nyc_subway_trnstl_template.apply(lambda row: \n",
    "                                                  Point(row['GTFS Longitude'], row['GTFS Latitude']), axis=1)\n",
    "nyc_subway_trnstl_template = gpd.GeoDataFrame(nyc_subway_trnstl_template, geometry='geometry', crs=nyc_nbhd_map.crs)\n",
    "\n",
    "nyc_subway_trnstl_template = gpd.tools.sjoin(nyc_subway_trnstl_template, nyc_nbhd_map, how='left').set_index('STATION')\n",
    "nyc_subway_trnstl_template = nyc_subway_trnstl_template[nyc_subway_trnstl_template.County != 'New York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_by_station_normed(df, station, nbhd_df, folder_name):              \n",
    "    station_df = df.loc[station][['ENTRIES', 'EXITS']]\n",
    "    \n",
    "    nbhd = nbhd_df.loc[station]['Name']\n",
    "    plot_name = station + ': ' + nbhd\n",
    "    \n",
    "    font = {'family' : 'normal',\n",
    "            'weight' : 'medium',\n",
    "            'size'   : 16}\n",
    "    matplotlib.rc('font', **font)\n",
    "      \n",
    "    s = pd.Series(np.arange(1,40))\n",
    "    station_df = station_df.reset_index(drop=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12,5))\n",
    "    matplotlib.pyplot.title(plot_name, fontsize=20, fontweight='heavy')\n",
    "\n",
    "    station_df.plot(ax=ax, marker='o')\n",
    "\n",
    "    ax.vlines(0, -2.5, 2.5)\n",
    "    ax.vlines(6, -2.5, 2.5)\n",
    "    ax.vlines(12, -2.5, 2.5)\n",
    "    ax.vlines(18, -2.5, 2.5)\n",
    "    ax.vlines(24, -2.5, 2.5)\n",
    "    ax.vlines(30, -2.5, 2.5)\n",
    "    ax.vlines(36, -2.5, 2.5)\n",
    "    ax.vlines(42, -2.5, 2.5)\n",
    " \n",
    "    if station_df.shape[0] == 42:\n",
    "        my_xticks = ['Mon','Tues','Wed','Thurs', 'Fri', 'Sat', 'Sun']\n",
    "        plt.xticks([3, 9, 15, 21, 27, 33, 39, 45], my_xticks)\n",
    "        plt.xlim([0, 41])\n",
    "    else:\n",
    "        my_xticks = ['Mon-Thurs', 'Fri', 'Sat', 'Sun']\n",
    "        plt.xticks([3, 9, 15, 21], my_xticks)\n",
    "        plt.xlim([0, 23])\n",
    "    \n",
    "    ax.axes.set_ylim=[-2.5, 2.5] \n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.ylabel('Count (Standardized)', fontsize=20)\n",
    "\n",
    "    save_name = plot_name.replace(' ', '')\n",
    "    save_name = save_name.replace('/', '-')\n",
    "    save_name = save_name.replace(':', '-')\n",
    "    plt.savefig(main_path + '/Results/Pics/' + folder_name + '/' + save_name + '.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "for this_station in df_1_by_wkday.STATION.unique():\n",
    "    plot_data_by_station_normed(df_1_by_wkday.set_index('STATION'), this_station, \n",
    "                                nyc_subway_trnstl_template, 'AllDays')\n",
    "    \n",
    "for this_station in df_1_by_wkday_type.STATION.unique():\n",
    "    plot_data_by_station_normed(df_1_by_wkday_type.set_index('STATION'), this_station, \n",
    "                                nyc_subway_trnstl_template, 'BlkDays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1 station with only entries first (for presentation)\n",
    "this_station = 'BAY RIDGE AV (R)'\n",
    "this_df = df_1_by_wkday.copy()\n",
    "this_df.loc[this_df.loc[:, 'STATION'] == this_station, 'EXITS'] = np.nan\n",
    "plot_data_by_station_normed(this_df.set_index('STATION'), this_station, \n",
    "                                nyc_subway_trnstl_template, 'AllDaysPreso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get map of blocks\n",
    "nyc_blk_map = gpd.read_file(main_path + '/Data/Locations/2010CensusBlocks/'\n",
    "                            'geo_export_8ec073df-ddd1-44a2-9db8-f3c2bc37058c.shp')\n",
    "nyc_blk_map = nyc_blk_map[nyc_blk_map.boro_name.isin(['Brooklyn', \n",
    "                                                        'Queens', 'Bronx', 'Staten Island'])]\n",
    "nyc_blk_map = nyc_blk_map.reset_index() # need to do this\n",
    "nyc_blk_map['Block'] = nyc_blk_map['bctcb2010'].apply(lambda x: str(x))\n",
    "nyc_blk_map = nyc_blk_map[['Block', 'geometry' ]]\n",
    "nyc_blk_map['Centroid'] = nyc_blk_map.centroid\n",
    "nyc_blk_map['LONG/LAT'] = [(c.x, c.y) for c in nyc_blk_map['Centroid']]\n",
    "\n",
    "# now for subway map template\n",
    "nyc_subway_trnstl_template = nyc_subway_trnstl_template.drop('index_right', axis=1)\n",
    "nyc_subway_trnstl_template = nyc_subway_trnstl_template.reset_index()\n",
    "nyc_subway_trnstl_template['LONG/LAT'] = [(c.x, c.y) for c in nyc_subway_trnstl_template['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spherical distance\n",
    "# Taken from here: \n",
    "# https://community.esri.com/groups/coordinate-reference-systems/blog/2017/10/05/haversine-formula\n",
    "def haversine(coord1: object, coord2: object):\n",
    "    import math\n",
    "\n",
    "    # Coordinates in decimal degrees (e.g. 2.89078, 12.79797)\n",
    "    lon1, lat1 = coord1\n",
    "    lon2, lat2 = coord2\n",
    "\n",
    "    R = 6371000  # radius of Earth in meters\n",
    "    phi_1 = math.radians(lat1)\n",
    "    phi_2 = math.radians(lat2)\n",
    "\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2.0) ** 2 + math.cos(phi_1) * math.cos(phi_2) * math.sin(delta_lambda / 2.0) ** 2\n",
    "    \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    meters = R * c  # output distance in meters\n",
    "    km = meters / 1000.0  # output distance in kilometers\n",
    "\n",
    "    meters = round(meters, 3)\n",
    "    km = round(km, 3)\n",
    "    miles = round(meters/1609.34,3)\n",
    "    \n",
    "    return miles\n",
    "    #print(f\"Distance: {meters} m\")\n",
    "    #print(f\"Distance: {km} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make distance matrix\n",
    "# REL: need to vectorize this\n",
    "nyc_blk_map = nyc_blk_map.reset_index()\n",
    "num_rows = nyc_blk_map.shape[0]\n",
    "\n",
    "# limit subway template to values that survived\n",
    "nyc_subway_trnstl_template = nyc_subway_trnstl_template[nyc_subway_trnstl_template \\\n",
    "                                                        .STATION.isin(df_1_by_wkday_type.STATION)]\n",
    "nyc_subway_trnstl_template = nyc_subway_trnstl_template.reset_index()\n",
    "num_cols = nyc_subway_trnstl_template.shape[0]\n",
    "\n",
    "dist_mat = np.zeros([num_rows, num_cols])\n",
    "for i in range(0, num_rows):\n",
    "    for j in range(0, num_cols):\n",
    "        dist_mat[i,j] = haversine(nyc_blk_map.loc[i, 'LONG/LAT'], nyc_subway_trnstl_template.loc[j, 'LONG/LAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter each block to subways within 3/4 of a mile\n",
    "# this might still have out of borough bleed over\n",
    "dist_mat_filt = 0.75 - dist_mat\n",
    "dist_mat_filt[dist_mat_filt < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rita/anaconda3/envs/insight/lib/python3.7/site-packages/geopandas/io/file.py:108: FionaDeprecationWarning: Use fiona.Env() instead.\n",
      "  with fiona.drivers():\n"
     ]
    }
   ],
   "source": [
    "# save stuff\n",
    "df_1_by_wkday.to_csv(main_path + '/Results/df_by_wkday.csv')\n",
    "df_1_by_wkday_type.to_csv(main_path + '/Results/df_by_wkday_type.csv')\n",
    "np.savetxt(main_path + '/Results/dist_block_to_station.csv', dist_mat_filt, delimiter=\",\")\n",
    "nyc_subway_trnstl_template.to_csv(main_path + '/Results/subway_station_info.csv')\n",
    "nyc_blk_map = nyc_blk_map.drop(['Centroid', 'index', 'LONG/LAT'], axis=1)\n",
    "nyc_blk_map.to_file(main_path + '/Results/nyc_blk_map.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
